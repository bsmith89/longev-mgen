{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {'acarbose': 'goldenrod', 'control': 'darkblue',\n",
    "             'UM': 'darkblue', 'UT': 'darkgreen',\n",
    "             'male': 'blue', 'female': 'magenta',\n",
    "             'C2013': 'blue', 'Glenn': 'red'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data\n",
    "\n",
    "con = sqlite3.connect('data/core.1.denorm.db')\n",
    "\n",
    "# Relative Abundance\n",
    "rrs_count = (pd.read_sql('SELECT * FROM rrs_taxon_count;',\n",
    "                         con=con, index_col=['extraction_id', 'sequence_id'])\n",
    "               .tally.unstack().fillna(0).astype(int))\n",
    "rabund = rrs_count.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "# Coverage\n",
    "bin_cvrg = (pd.read_sql(\"\"\"\n",
    "SELECT bin_id, extraction_id, SUM(coverage) AS coverage\n",
    "FROM bin_coverage\n",
    "JOIN library USING (library_id)\n",
    "GROUP BY bin_id, extraction_id;\"\"\",\n",
    "                        con=con, index_col=['extraction_id', 'bin_id'])\n",
    "              .coverage.unstack().fillna(0).apply(lambda x: x / x.sum(), axis=1))\n",
    "\n",
    "# Only keep shared extractions\n",
    "extractions = set(rabund.index) & set(bin_cvrg.index)\n",
    "rabund = rabund.loc[extractions]\n",
    "bin_cvrg = bin_cvrg.loc[extractions]\n",
    "\n",
    "# Phylotypes\n",
    "phylotype = pd.read_sql('SELECT sequence_id, otu_id FROM rrs_taxon_count GROUP BY sequence_id;',\n",
    "                       con=con, index_col='sequence_id')\n",
    "name_map = {}\n",
    "for otu, d in (pd.DataFrame({'mean_rabund': rabund.mean(),\n",
    "                             'otu_id': phylotype.otu_id})\n",
    "                 .sort_values('mean_rabund',\n",
    "                              ascending=False)\n",
    "                 .groupby('otu_id')):\n",
    "    for i, sequence_id in enumerate(d.index, start=1):\n",
    "        name_map[sequence_id] = '{}_{}'.format(otu, i)\n",
    "phylotype['name'] = pd.Series(name_map)\n",
    "phylotype['mean_rabund'] = rabund.mean()\n",
    "\n",
    "contig_bin = pd.read_sql(\"SELECT * FROM contig_bin\", con=con, index_col='contig_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library = pd.read_table('meta/library.tsv', index_col='library_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTU Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_sql('SELECT sequence_id, phylum_, class_, order_, family_, genus_ FROM rrs_taxonomy;',\n",
    "                       con=con, index_col='sequence_id').rename(phylotype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select abundant taxa and bins\n",
    "# TODO: Set these threshold as parameters\n",
    "major_taxa = phylotype.index[phylotype.mean_rabund > 0.0001]\n",
    "major_bins = bin_cvrg.columns[bin_cvrg.mean() > 0.0001]\n",
    "d_rabund = rabund[major_taxa].copy()\n",
    "d_rabund['other'] = rabund.drop(columns=major_taxa).sum(1)\n",
    "d_rabund.rename(columns=phylotype.name, inplace=True)\n",
    "d_cvrg = bin_cvrg[major_bins].copy()\n",
    "d_cvrg['other'] = bin_cvrg.drop(columns=major_bins).sum(1)\n",
    "\n",
    "d_rabund.shape, d_cvrg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rabund.mean().to_frame(name='mean_rabund').join(taxonomy).sort_values('mean_rabund', ascending=False)[lambda x: x.family_ == 'Muribaculaceae'].head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabinning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_fit = PLSCanonical(scale=False, n_components=40).fit(d_cvrg.apply(np.sqrt), d_rabund.apply(np.sqrt))\n",
    "bin_otu_contrib = pd.DataFrame((pls_fit.x_loadings_ @ pls_fit.y_loadings_.T),\n",
    "                       index=d_cvrg.columns, columns=d_rabund.columns).rename(columns=phylotype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_filter = lambda x: x.quantile(0.95) > 0.01\n",
    "\n",
    "taxa_of_interest = sorted(d_rabund.loc[:, tax_filter].rename(columns=phylotype.name).columns)\n",
    "if 'other' in taxa_of_interest:\n",
    "    del taxa_of_interest[taxa_of_interest.index('other')]\n",
    "print(len(taxa_of_interest))\n",
    "\n",
    "\n",
    "\n",
    "factor = 1/3\n",
    "\n",
    "_hits = {}\n",
    "for tax in taxa_of_interest:\n",
    "    top_score = bin_otu_contrib[tax].max()\n",
    "    print(tax, top_score)\n",
    "    _hits[tax] = list((bin_otu_contrib[tax].sort_values(ascending=False) > top_score * factor)[lambda x: x].index)\n",
    "    \n",
    "print()\n",
    "for tax in _hits:\n",
    "    print(tax, _hits[tax])\n",
    "\n",
    "all_hits = set(chain(*_hits.values()))\n",
    "\n",
    "a = sns.clustermap(bin_otu_contrib.loc[all_hits, taxa_of_interest].rename(columns=phylotype.name), robust=True,\n",
    "                   figsize=(14, 18), col_cluster=False, cmap='coolwarm', center=0)\n",
    "\n",
    "ax = a.fig.get_axes()[2]\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabin Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1 / OTU-1 / Otu0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/3\n",
    "otus = ['Otu0001_1', 'Otu0001_2', 'Otu0001_3',\n",
    "        'Otu0001_4', 'Otu0001_5'\n",
    "       ]\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00378'].sort_values('length', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '536619', '1830468',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=40,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              max_iter=int(1e3),\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 35\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "# INSTRUCTION: Pick a linscale that best contrasts enriched and depleted groups.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[22, ]))\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "# \"UM\" Strain\n",
    "with open('chkpt/core.a.mags/B1A.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([0, 6, 9, 13, 15, 29])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B1A.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(list(cvrg_norm[extraction_meta.site == 'UM'].index))\n",
    "    extraction_ids -= set([])\n",
    "    extraction_ids |= set(['EXT-0237', 'EXT-0226', 'EXT-0243', 'EXT-0045', 'EXT-0100', 'EXT-0094', 'EXT-0107'\n",
    "                          ])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B1A.g.trusted_depth.tsv', sep='\\t', header=False)\n",
    "\n",
    "# UT Strain\n",
    "with open('chkpt/core.a.mags/B1B.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([6, 9, 13, 21, 29])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B1B.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(list(cvrg_norm[extraction_meta.site == 'UT'].index))\n",
    "    extraction_ids -= set(['EXT-0237', 'EXT-0226', 'EXT-0243', 'EXT-0045', 'EXT-0100', 'EXT-0094', 'EXT-0107',\n",
    "                           'EXT-0081','EXT-0246', 'EXT-0192'])\n",
    "#    extraction_ids |= set(['EXT-0421'])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B1B.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2 / OTU-4 / Otu0007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/4\n",
    "otus = ['Otu0007_1', 'Otu0007_2']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00225'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '35357', '1023797',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "nn = range(1, 40)\n",
    "scores = []\n",
    "for n in nn:\n",
    "    score = BayesianGaussianMixture(n,\n",
    "                                  covariance_type='diag',\n",
    "    #                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "    #                              weight_concentration_prior=10,\n",
    "                                  random_state=1,\n",
    "                                 ).fit(cluster_data.T).score(cluster_data.T)\n",
    "    scores.append(score)\n",
    "plt.plot(nn, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=20,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 50\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "# sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "#                col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d, vmin=vmin, vmax=vmax,\n",
    "               col_cluster=True, robust=True, cmap=cmap, norm=norm, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B2.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([12, 15, 0, 18])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B2.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B2.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3 / OTU-? / Otu0009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/3\n",
    "otus = ['Otu0009_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00800'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '1621700', '2880790',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "nn = range(1, 40)\n",
    "scores = []\n",
    "for n in nn:\n",
    "    score = BayesianGaussianMixture(n,\n",
    "                                  covariance_type='diag',\n",
    "    #                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "    #                              weight_concentration_prior=10,\n",
    "                                  random_state=1,\n",
    "                                 ).fit(cluster_data.T).score(cluster_data.T)\n",
    "    scores.append(score)\n",
    "plt.plot(nn, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 50\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=0.9)\n",
    "\n",
    "sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B3.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([6])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B3.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B3.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B4 / OTU-? / Otu0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/3\n",
    "otus = ['Otu0005_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin01956'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '156806', '2140623',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.95\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "nn = range(1, 40)\n",
    "scores = []\n",
    "for n in nn:\n",
    "    score = BayesianGaussianMixture(n,\n",
    "                                  covariance_type='diag',\n",
    "    #                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "    #                              weight_concentration_prior=10,\n",
    "                                  random_state=1,\n",
    "                                 ).fit(cluster_data.T).score(cluster_data.T)\n",
    "    scores.append(score)\n",
    "plt.plot(nn, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 50\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B4.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([0, 6, 7])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B4.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set(['EXT-0107', 'EXT-0210'])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B4.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B5 / OTU-? / Otu0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/3\n",
    "otus = ['Otu0004_1', 'Otu0004_2']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin01354'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '3759879', '586388',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "nn = range(1, 40)\n",
    "scores = []\n",
    "for n in nn:\n",
    "    score = BayesianGaussianMixture(n,\n",
    "                                  covariance_type='diag',\n",
    "    #                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "    #                              weight_concentration_prior=10,\n",
    "                                  random_state=1,\n",
    "                                 ).fit(cluster_data.T).score(cluster_data.T)\n",
    "    scores.append(score)\n",
    "plt.plot(nn, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=20,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 50\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "# UT Strain\n",
    "with open('chkpt/core.a.mags/B5.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([0, 2, 7, 8, 13, 15])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B5.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(list(cvrg_norm[extraction_meta.site == 'UT'].index))\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B5.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B6 / OTU-? / Otu0049"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/5\n",
    "otus = ['Otu0049_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin01484'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '975319', '3176163',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=50,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 80\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "sns.clustermap(d, vmin=vmin, vmax=vmax,\n",
    "               col_cluster=True, robust=True, cmap=cmap, norm=norm, metric='cosine')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B6.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([22, 4])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B6.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set(['EXT-0405', 'EXT-0238', 'EXT-0045'])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B6.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B7 / OTU-? / Otu0017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/5\n",
    "otus = ['Otu0017_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00280'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '1819450', '988141',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=50,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 20\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "\n",
    "    ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[46, 41, 10])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "# sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "#                col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')\n",
    "d[extraction_meta.site == 'UM']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "# UT Strain\n",
    "with open('chkpt/core.a.mags/B7.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([36, 25])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B7.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(list(cvrg_norm[extraction_meta.site == 'UT'].index))\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B7.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B8 / OTU-? / Otu0013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/2\n",
    "otus = ['Otu0013_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00599'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '766897', '2518556',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 20\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "\n",
    "    ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "# sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "#                col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d, vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B8.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([1])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B8.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B8.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B9 / OTU-? / Otu0105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the closely related OTUs to pool together.\n",
    "# INSTRUCTION: Pick a *keep_thresh_factor* to choose the bins associated with these OTUs that\n",
    "# will be considered below.\n",
    "\n",
    "keep_thresh_factor = 1/2\n",
    "otus = ['Otu0105_1']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "#cvrg = cvrg.div(extraction_meta.coverage, axis=0).loc[d_rabund.index]\n",
    "cvrg = cvrg.loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(rabund\n",
    "     .rename(columns=phylotype.name)\n",
    "     .groupby(extraction_meta.site)\n",
    "     [otus]\n",
    "     .mean())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "contig_cvrg_similarity = pd.DataFrame(1 - sp.spatial.distance.squareform(sp.spatial.distance.pdist(cvrg.T, metric='cosine')),\n",
    "                                      index=cvrg.columns, columns=cvrg.columns)\n",
    "\n",
    "sns.clustermap(contig_cvrg_similarity, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *bin_id* to help picking a seed contig.\n",
    "\n",
    "contig_meta[lambda x: x.bin_id == 'bin00573'].sort_values('length', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select a *seed* contig whose coverage seems to best reflect the relative\n",
    "# abundance of the OTUs of interest and find a second contig with matching coverage\n",
    "# to *compare* as a double-check.\n",
    "# INSTRUCTION: Select *trusted_contigs* threshold that selects the contigs used to normalize coverage.\n",
    "# INSTRUCTION: Select *trusted_extractions* threshold that selects the extractions to be considered below.\n",
    "\n",
    "seed, compare = '588300', '1679634',\n",
    "assert seed in contig_ids\n",
    "assert compare in contig_ids\n",
    "plt.scatter(seed, compare, c='rabund',\n",
    "            data=d_rabund[otus].sum(1).to_frame(name='rabund').join(cvrg).sort_values('rabund'),\n",
    "            cmap='coolwarm')\n",
    "plt.plot([0, 1e3], [0, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "plt.yscale('symlog')\n",
    "plt.xscale('symlog')\n",
    "\n",
    "contig_thresh = 0.99\n",
    "extract_thresh = 0.5\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > contig_thresh].index\n",
    "\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > extract_thresh].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Select the *n_components* (usually between 10 and 100) to best resolve the contigs into groups.\n",
    "# INSTRUCTION: Pick a *contam_threshold* that excludes as many groups as possible without excluding any that\n",
    "# should be included.\n",
    "\n",
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=30,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "_group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(_group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(_group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(_group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage * np.log(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = _group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 50\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.min().min()-1e-1), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "\n",
    "    ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a.join(b, how='outer').fillna(0).astype(int)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.min()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.5, 0), (0, 0.8, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick groups you want to drop from visualization.\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]\n",
    "     .drop(columns=[])\n",
    "    )\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 20, cm, SymLogNorm(linthresh=1, linscale=2)\n",
    "\n",
    "# sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "#                col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d, vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm, metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Pick curated contig groups and extractions for each MAG.\n",
    "\n",
    "with open('chkpt/core.a.mags/B9.g.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([1, 4])].index:\n",
    "        print(contig_id, file=handle)\n",
    "with open('chkpt/core.a.mags/B9.g.library.list', 'w') as handle:\n",
    "    extraction_ids = set(trusted_extractions)\n",
    "    extraction_ids -= set([])\n",
    "    for library_id in library[library.extraction_id.isin(extraction_ids)].index:\n",
    "        print(library_id, file=handle)\n",
    "library.join(cvrg, on='extraction_id')[trusted_contigs].sum(1).to_csv('chkpt/core.a.mags/B9.g.trusted_depth.tsv', sep='\\t', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "102px",
    "width": "189px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}