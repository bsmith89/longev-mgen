{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: The ultimate algorithm\n",
    "\n",
    "1.  Select the seq_rabund and bin_cvrg data\n",
    "1.  Normalize each matrix to sum to 1 across samples\n",
    "1.  Cross-validation on PLSCanonical to pick n_components\n",
    "1.  Fit PLSCanonical\n",
    "1.  Calculate contribution scores (x_loadings @ y_loadings)\n",
    "1.  Identify and drop Bins for which many seqs are correlated\n",
    "1.  Re-normalize, Re-CV, and re-fit PLSCanonical with reduced data?\n",
    "1.  For each phylotype of interest (top n seqs?), fit a t-distribution to the contribution scores\n",
    "1.  Pick the bins so as to stay under a false-positive threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data\n",
    "\n",
    "con = sqlite3.connect('res/core.1.denorm.db')\n",
    "\n",
    "# Relative Abundance\n",
    "rrs_count = (pd.read_sql('SELECT * FROM rrs_taxon_count;',\n",
    "                         con=con, index_col=['extraction_id', 'sequence_id'])\n",
    "               .tally.unstack().fillna(0).astype(int))\n",
    "rabund = rrs_count.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "# Coverage\n",
    "bin_cvrg = (pd.read_sql(\"\"\"\n",
    "SELECT bin_id, extraction_id, SUM(coverage) AS coverage\n",
    "FROM bin_coverage\n",
    "JOIN library USING (library_id)\n",
    "GROUP BY bin_id, extraction_id;\"\"\",\n",
    "                        con=con, index_col=['extraction_id', 'bin_id'])\n",
    "              .coverage.unstack().fillna(0).apply(lambda x: x / x.sum(), axis=1))\n",
    "\n",
    "# Only keep shared extractions\n",
    "extractions = set(rabund.index) & set(bin_cvrg.index)\n",
    "rabund = rabund.loc[extractions]\n",
    "bin_cvrg = bin_cvrg.loc[extractions]\n",
    "\n",
    "# Taxonomy\n",
    "taxonomy = pd.read_sql('SELECT sequence_id, otu_id FROM rrs_taxon_count GROUP BY sequence_id;',\n",
    "                       con=con, index_col='sequence_id')\n",
    "name_map = {}\n",
    "for otu, d in (pd.DataFrame({'mean_rabund': rabund.mean(),\n",
    "                             'otu_id': taxonomy.otu_id})\n",
    "                 .sort_values('mean_rabund',\n",
    "                              ascending=False)\n",
    "                 .groupby('otu_id')):\n",
    "    for i, sequence_id in enumerate(d.index, start=1):\n",
    "        name_map[sequence_id] = '{}_{}'.format(otu, i)\n",
    "taxonomy['name'] = pd.Series(name_map)\n",
    "taxonomy['mean_rabund'] = rabund.mean()\n",
    "\n",
    "\n",
    "# Select abundant taxa and bins\n",
    "# TODO: Set these threshold as parameters\n",
    "major_taxa = taxonomy.index[taxonomy.mean_rabund > 0.0001]\n",
    "major_bins = bin_cvrg.columns[bin_cvrg.mean() > 0.0001]\n",
    "d_rabund = rabund[major_taxa].copy()\n",
    "d_rabund['other'] = rabund.drop(columns=major_taxa).sum(1)\n",
    "d_rabund.rename(columns=taxonomy.name, inplace=True)\n",
    "d_cvrg = bin_cvrg[major_bins].copy()\n",
    "d_cvrg['other'] = bin_cvrg.drop(columns=major_bins).sum(1)\n",
    "\n",
    "d_rabund.shape, d_cvrg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(X, Y, model, k, random_state=None):\n",
    "    n = len(X.index) // k\n",
    "    assert n > 2\n",
    "    order = list(X.sample(frac=1, random_state=random_state).index)\n",
    "    scores = []\n",
    "    for i in range(k):\n",
    "        outgroup = order[n*i:n*i+n]\n",
    "        fit = model.fit(X.drop(outgroup), Y.drop(outgroup))\n",
    "        scores.append(fit.score(X.loc[outgroup], Y.loc[outgroup]))\n",
    "    return scores\n",
    "\n",
    "for n_components in [10, 15, 20, 25, 30, 35, 40, 100, 500]:\n",
    "    model = PLSCanonical(scale=False, n_components=n_components)\n",
    "    print(n_components,\n",
    "          np.mean(crossval(d_cvrg.apply(np.sqrt),\n",
    "                           d_rabund.apply(np.sqrt),\n",
    "                           model, 4)\n",
    "                 )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick n_components\n",
    "\n",
    "n_components = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = PLSCanonical(scale=False, n_components=n_components).fit(d_cvrg.apply(np.sqrt), d_rabund.apply(np.sqrt))\n",
    "contrib = pd.DataFrame((fit.x_loadings_ @ fit.y_loadings_.T),\n",
    "                       index=d_cvrg.columns, columns=d_rabund.columns).rename(columns=taxonomy.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_mean_abund = 0.008\n",
    "tax_filter = lambda x: x.quantile(0.90) > 0.02\n",
    "\n",
    "taxa_of_interest = sorted(d_rabund.loc[:, tax_filter].rename(columns=taxonomy.name).columns)\n",
    "if 'other' in taxa_of_interest:\n",
    "    del taxa_of_interest[taxa_of_interest.index('other')]\n",
    "len(taxa_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 0.5\n",
    "\n",
    "hits = {}\n",
    "for tax in taxa_of_interest:\n",
    "    top_score = contrib[tax].max()\n",
    "    print(tax, top_score)\n",
    "    hits[tax] = list((contrib[tax].sort_values(ascending=False) > top_score * factor)[lambda x: x].index)\n",
    "    \n",
    "print()\n",
    "for tax in hits:\n",
    "    print(tax, hits[tax])\n",
    "\n",
    "all_hits = set(chain(*hits.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sns.clustermap(contrib.loc[all_hits, taxa_of_interest].rename(columns=taxonomy.name), robust=True,\n",
    "                   figsize=(14, 18), col_cluster=False, cmap='coolwarm', center=0)\n",
    "\n",
    "ax = a.fig.get_axes()[2]\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.savefig('heatmap.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}