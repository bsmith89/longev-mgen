{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sklearn.cross_decomposition import PLSCanonical\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {'acarbose': 'goldenrod', 'control': 'darkblue',\n",
    "             'UM': 'darkblue', 'UT': 'darkgreen',\n",
    "             'male': 'blue', 'female': 'magenta',\n",
    "             'C2013': 'blue', 'Glenn': 'red'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the data\n",
    "\n",
    "con = sqlite3.connect('res/core.1.denorm.db')\n",
    "\n",
    "# Relative Abundance\n",
    "rrs_count = (pd.read_sql('SELECT * FROM rrs_taxon_count;',\n",
    "                         con=con, index_col=['extraction_id', 'sequence_id'])\n",
    "               .tally.unstack().fillna(0).astype(int))\n",
    "rabund = rrs_count.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "# Coverage\n",
    "bin_cvrg = (pd.read_sql(\"\"\"\n",
    "SELECT bin_id, extraction_id, SUM(coverage) AS coverage\n",
    "FROM bin_coverage\n",
    "JOIN library USING (library_id)\n",
    "GROUP BY bin_id, extraction_id;\"\"\",\n",
    "                        con=con, index_col=['extraction_id', 'bin_id'])\n",
    "              .coverage.unstack().fillna(0).apply(lambda x: x / x.sum(), axis=1))\n",
    "\n",
    "# Only keep shared extractions\n",
    "extractions = set(rabund.index) & set(bin_cvrg.index)\n",
    "rabund = rabund.loc[extractions]\n",
    "bin_cvrg = bin_cvrg.loc[extractions]\n",
    "\n",
    "# Phylotypes\n",
    "phylotype = pd.read_sql('SELECT sequence_id, otu_id FROM rrs_taxon_count GROUP BY sequence_id;',\n",
    "                       con=con, index_col='sequence_id')\n",
    "name_map = {}\n",
    "for otu, d in (pd.DataFrame({'mean_rabund': rabund.mean(),\n",
    "                             'otu_id': phylotype.otu_id})\n",
    "                 .sort_values('mean_rabund',\n",
    "                              ascending=False)\n",
    "                 .groupby('otu_id')):\n",
    "    for i, sequence_id in enumerate(d.index, start=1):\n",
    "        name_map[sequence_id] = '{}_{}'.format(otu, i)\n",
    "phylotype['name'] = pd.Series(name_map)\n",
    "phylotype['mean_rabund'] = rabund.mean()\n",
    "\n",
    "contig_bin = pd.read_sql(\"SELECT * FROM contig_bin\", con=con, index_col='contig_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OTU Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxonomy = pd.read_sql('SELECT sequence_id, phylum_, class_, order_, family_, genus_ FROM taxonomy;',\n",
    "                       con=con, index_col='sequence_id').rename(phylotype.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select abundant taxa and bins\n",
    "# TODO: Set these threshold as parameters\n",
    "major_taxa = phylotype.index[phylotype.mean_rabund > 0.0001]\n",
    "major_bins = bin_cvrg.columns[bin_cvrg.mean() > 0.0001]\n",
    "d_rabund = rabund[major_taxa].copy()\n",
    "d_rabund['other'] = rabund.drop(columns=major_taxa).sum(1)\n",
    "d_rabund.rename(columns=phylotype.name, inplace=True)\n",
    "d_cvrg = bin_cvrg[major_bins].copy()\n",
    "d_cvrg['other'] = bin_cvrg.drop(columns=major_bins).sum(1)\n",
    "\n",
    "d_rabund.shape, d_cvrg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rabund.mean().to_frame(name='mean_rabund').join(taxonomy).sort_values('mean_rabund', ascending=False).loc[['Otu0058_1', 'Otu0041_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_rabund.mean().to_frame(name='mean_rabund').join(taxonomy).sort_values('mean_rabund', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabinning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find combinations of bins that are likely to have contigs from the same genome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLSCanonical is a version of CanonicalCorrespondanceAnalysis in which two different matrices\n",
    "are cross-decomposed.\n",
    "We're modeling both the relative abundance of each OTU and the coverage of each bin as different\n",
    "transformation of the same underlying latent space (actual genome density).\n",
    "We then use the cross-product of the two loading vectors to get an aggregate estimate of the\n",
    "relationship between bins and OTUs.\n",
    "The key parameter in this machine-learning approach is **n_components**, in other words the\n",
    "complexity of the underlying latent state.\n",
    "With a small value of **n_components** similar OTUs will get more and more similar, potentially\n",
    "introducing noise into the metabin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pls_fit = PLSCanonical(scale=False, n_components=40).fit(d_cvrg.apply(np.sqrt), d_rabund.apply(np.sqrt))\n",
    "bin_otu_contrib = pd.DataFrame((pls_fit.x_loadings_ @ pls_fit.y_loadings_.T),\n",
    "                       index=d_cvrg.columns, columns=d_rabund.columns).rename(columns=phylotype.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll plot the most important results.\n",
    "The bin hits for a given taxon are identified as the bin with the highest \"contribution score\" in that column,\n",
    "along with all other bins with a score greater than some fraction, **factor** of that score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_filter = lambda x: x.quantile(0.95) > 0.01\n",
    "\n",
    "taxa_of_interest = sorted(d_rabund.loc[:, tax_filter].rename(columns=phylotype.name).columns)\n",
    "if 'other' in taxa_of_interest:\n",
    "    del taxa_of_interest[taxa_of_interest.index('other')]\n",
    "print(len(taxa_of_interest))\n",
    "\n",
    "factor = 1/3\n",
    "\n",
    "_hits = {}\n",
    "for tax in taxa_of_interest:\n",
    "    top_score = bin_otu_contrib[tax].max()\n",
    "#    print(tax, top_score)\n",
    "    _hits[tax] = list((bin_otu_contrib[tax].sort_values(ascending=False) > top_score * factor)[lambda x: x].index)\n",
    "    \n",
    "#print()\n",
    "#for tax in _hits:\n",
    "#    print(tax, _hits[tax])\n",
    "\n",
    "all_hits = set(chain(*_hits.values()))\n",
    "\n",
    "for_plotting = bin_otu_contrib.loc[all_hits, taxa_of_interest].rename(columns=phylotype.name)\n",
    "for_plotting[for_plotting < 0] = 0\n",
    "a = sns.clustermap(for_plotting, robust=True,\n",
    "                   figsize=(14, 18), col_cluster=False, cmap='Reds')\n",
    "\n",
    "ax = a.fig.get_axes()[2]\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metabin Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to do some filtering of the contigs in the metabins that were grouped together.\n",
    "That's because the binning was inherently noisy and may include sequence from other genomes.\n",
    "The challenge is figuring out how to identify contigs to be removed.\n",
    "I can't just go on overall coverage because I may get false negatives, for instance sequences that are only present in some strains, sequences that are missassembled, or sequences at higher copy numbers (e.g. 16S).\n",
    "I might also get some false positives,\n",
    "sequences that coincidentally have mean coverage close to the expected level.\n",
    "I also can't use raw coverage covariance across libraries\n",
    "(although this would fix many of the above problems), because I'll get false negatives\n",
    "for genome components that are not universally present (the \"accessory genome\").\n",
    "\n",
    "The approach I'm actually going to take will look for clusters of contigs that behave similarly\n",
    "across samples.  That way I'm not saying that everything will behave like the core genome,\n",
    "but I'm also relying on coverage covariance as a powerful indicator of shared genome membership.\n",
    "\n",
    "I will then filter these groups of contigs based on how much they look like some other organism.\n",
    "Looking like some other organism means that they have large swings in coverage across samples\n",
    "while still having consistent coverage across contigs.\n",
    "\n",
    "In terms of a statistic to measure this, I'll use the ratio of standard-deviation of mean contig abundance\n",
    "(higher when different samples look very different)\n",
    "to the mean standard-deviation of contig abundance (lower when the group of contigs are all acting the same\n",
    "in each sample).\n",
    "Since this has a tendency to exclude contig groups that don't have a lot of sequence (and are therefore\n",
    "noisier), I'll adjust this ratio by multiplying it by the log total contig length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTU-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab the set of bins that were associated with any of the top four Otu0001 phylotypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_thresh_factor = 1/3\n",
    "otus = ['Otu0001_1', 'Otu0001_2', 'Otu0001_3', 'Otu0001_4']\n",
    "bins = set()\n",
    "for otu in otus:\n",
    "    max_contrib = bin_otu_contrib[otu].max()\n",
    "    bins |= set(bin_otu_contrib[otu][lambda x: x > max_contrib * keep_thresh_factor].index)\n",
    "    \n",
    "print(bins)\n",
    "contig_ids = set(contig_bin[lambda x: x.bin_id.isin(bins)].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the necessary data and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_ids_sql = '\"' + '\", \"'.join(contig_ids) + '\"'\n",
    "\n",
    "cvrg = pd.read_sql(\"\"\"\n",
    "SELECT extraction_id, contig_id, SUM(coverage) AS coverage\n",
    "FROM contig_coverage\n",
    "JOIN library USING (library_id)\n",
    "WHERE contig_id IN ({})\n",
    "GROUP BY extraction_id, contig_id\n",
    "                   \"\"\".format(contig_ids_sql), con=con,\n",
    "                   index_col=['extraction_id', 'contig_id']).coverage.unstack('contig_id', fill_value=0)\n",
    "\n",
    "extraction_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM extraction\n",
    "JOIN sample USING (sample_id)\n",
    "JOIN mouse USING (mouse_id)\n",
    "JOIN (SELECT extraction_id, SUM(mapping_count) AS mapping_count\n",
    "      FROM library_total_nucleotides_mapping\n",
    "      JOIN library USING (library_id)\n",
    "      GROUP BY extraction_id) USING (extraction_id)\n",
    "                               \"\"\", con=con, index_col='extraction_id')\n",
    "\n",
    "contig_meta = pd.read_sql(\"\"\"\n",
    "SELECT *\n",
    "FROM contig_bin\n",
    "JOIN contig USING (contig_id)\n",
    "WHERE contig_id IN ({})\n",
    "                          \"\"\".format(contig_ids_sql),\n",
    "                         con=con, index_col='contig_id')\n",
    "\n",
    "cvrg = cvrg.div(extraction_meta.mapping_count, axis=0).loc[d_rabund.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to normalize the coverages since each sample has different amounts of\n",
    "each genome.\n",
    "My approach to normalization is to normalize to the coverage of sequence that is\n",
    "ubiquitous and in a single copy in all genomes (i.e. \"core\" sequence).\n",
    "The result is a good estimate of copies per core-genome (which could be\n",
    "either greater than (multiple copies) or less than 1 (contig is missassembled),\n",
    "but is frequently very close to 1).\n",
    "\n",
    "I estimate this coverage by finding contigs that are indisputably\n",
    "in the \"core genome\" and normalize everything to the mean coverage of these contigs.\n",
    "The way I find these \"core\" contigs is by picking a \"seed\" contig and then looking for\n",
    "all of the contigs with coverages that are SUPER closely correlated with the seed (like r > 0.99).\n",
    "How I pick the seed is more of an art.\n",
    "Right now I'm picking the seed by finding the contig with the highest correlation to the\n",
    "16S relative abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(cvrg['core-k161_641135']*1e6, d_rabund[otus].sum(1).loc[cvrg.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0]).sort_values().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rrs_corr = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0]).sort_values()\n",
    "rrs_corr.name = 'rrs_corr'\n",
    "contig_meta.join(rrs_corr).sort_values('rrs_corr')[lambda x: x.length > 10000].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'core-k161_375476'\n",
    "assert seed in contig_ids\n",
    "#assert compare in contig_ids\n",
    "plt.scatter(cvrg[seed]*1e6, d_rabund[otus].sum(1).loc[cvrg.index])\n",
    "#plt.plot([-1e3, 1e3], [-1e3, 1e3], c='k', lw=1, scalex=False, scaley=False)\n",
    "\n",
    "trusted_contigs = cvrg.apply(lambda x: sp.stats.pearsonr(cvrg[seed], x)[0])[lambda x: x > 0.99].index\n",
    "trusted_extractions = (cvrg[trusted_contigs].mean(1) / cvrg[trusted_contigs].std(1))[lambda x: x > 2].index\n",
    "\n",
    "print('{} trusted contigs and {} trusted extractions identified'.format(len(trusted_contigs), len(trusted_extractions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvrg_norm = cvrg.div(cvrg[trusted_contigs].mean(1), axis=0)\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='with_untrusted', alpha=0.8)\n",
    "cvrg_norm = cvrg_norm.loc[trusted_extractions]\n",
    "_ = plt.hist(np.log(cvrg_norm.mean()), bins=np.linspace(-4, 6), label='without_untrusted', alpha=0.8)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = np.sqrt(cvrg_norm)\n",
    "\n",
    "nn = range(1, 40)\n",
    "scores = []\n",
    "for n in nn:\n",
    "    score = BayesianGaussianMixture(n,\n",
    "                                  covariance_type='diag',\n",
    "    #                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "    #                              weight_concentration_prior=10,\n",
    "                                  random_state=1,\n",
    "                                 ).fit(cluster_data.T).score(cluster_data.T)\n",
    "    scores.append(score)\n",
    "plt.plot(nn, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgm = BayesianGaussianMixture(20,\n",
    "                              covariance_type='diag',\n",
    "#                              weight_concentration_prior_type='dirichlet_distribution',\n",
    "#                              weight_concentration_prior=10,\n",
    "                              random_state=1,\n",
    "                             ).fit(cluster_data.T)\n",
    "group_assign = pd.Series(bgm.predict(cluster_data.T), index=cvrg_norm.columns)\n",
    "group_cvrg = cvrg_norm.groupby(group_assign, axis='columns').mean().mean().to_frame(name='group_mean_mean_coverage')\n",
    "group_cvrg['group_mean_std_coverage'] = cvrg_norm.groupby(group_assign, axis='columns').std().mean()\n",
    "group_cvrg['group_std_mean_coverage'] = cvrg_norm.groupby(group_assign, axis='columns').mean().std()\n",
    "group_cvrg['group_max_coverage'] = cvrg_norm.groupby(group_assign, axis='columns').max().max()\n",
    "group_cvrg['total_length'] = contig_meta.groupby(group_assign).length.sum()\n",
    "group_cvrg['contamination_score'] = group_cvrg.group_std_mean_coverage / group_cvrg.group_mean_std_coverage**2   # * np.sqrt(group_cvrg.total_length)\n",
    "group_cvrg.index.name = 'group'\n",
    "group_assign = group_assign.to_frame(name='group').join(group_cvrg, on='group')\n",
    "group_assign['bin_id'] = contig_meta.bin_id\n",
    "group_assign['length'] = contig_meta.length\n",
    "group_assign.sort_values(['contamination_score', 'length'], ascending=[True, False], inplace=True)\n",
    "# order = group_assign.index\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "\n",
    "for des, d in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "#    color = None\n",
    "    _ = ax.plot(d[group_assign.index].values.T, lw=1, alpha=0.25, color=color)\n",
    "#_ = ax.plot(group_assign.group_mean_coverage.values, color='k')\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "\n",
    "group_assign['contig_index'] = range(group_assign.shape[0])\n",
    "group_order = \\\n",
    "    (group_assign.groupby('group').contig_index\n",
    "                         .apply(lambda x: pd.Series({'middle': x.mean(),\n",
    "                                                     'left': x.min(),\n",
    "                                                     'right': x.max()}))).unstack().sort_values('left')\n",
    "contam_threshold = 20\n",
    "for inx, d in group_order.iterrows():\n",
    "    if group_cvrg.loc[inx].contamination_score > contam_threshold:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    elif group_cvrg.loc[inx].isna().contamination_score:\n",
    "        ax.axvline(d.left - 0.5, color='r', lw=0.5)\n",
    "        continue\n",
    "    else:\n",
    "        ax.axvline(d.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d.middle, cvrg_norm.max().max()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg.loc[inx].total_length),\n",
    "                xy=(d.middle, cvrg_norm.max().max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "ax.set_yscale('symlog', linthreshy=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "group_cvrg.sort_values('contamination_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (group_assign\n",
    "         [lambda x: x.group.isin(group_cvrg[lambda x: x.contamination_score < contam_threshold].index)]\n",
    "         .groupby(['bin_id', 'group']).length.sum().unstack(fill_value=0))\n",
    "b = (group_assign\n",
    "                   [lambda x: x.group.isin(group_cvrg[lambda x: ( x.contamination_score > contam_threshold)\n",
    "                                                                | x.contamination_score.isna()\n",
    "                                                     ].index)]\n",
    "                   .groupby('bin_id').length.sum())\n",
    "b.name = 'contam'\n",
    "a.join(b, how='outer').fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cvrg_norm.groupby([extraction_meta.site, extraction_meta.treatment]).count().iloc[:,0])\n",
    "\n",
    "(cvrg_norm.groupby(group_assign.group, axis='columns').mean()\n",
    "          .groupby([extraction_meta.site, extraction_meta.treatment]).mean()).loc[:, group_cvrg[lambda x: x.contamination_score < contam_threshold].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.axhline(y=1, color='k', linestyle='--')\n",
    "artists = []\n",
    "plotting_order = []\n",
    "for des, d0 in cvrg_norm.groupby(extraction_meta.site):\n",
    "    color = color_map[des]\n",
    "    des_artists = ax.plot(d0[group_assign[lambda x: x.contamination_score < contam_threshold].index].values.T,\n",
    "                          lw=1, alpha=0.1, color=color)\n",
    "    artists.extend(des_artists)\n",
    "    plotting_order.extend(d0.index)\n",
    "#original_colors = {a: a.get_color() for a in artists}\n",
    "original_lw = {a: a.get_linewidth() for a in artists}\n",
    "original_alpha = {a: a.get_alpha() for a in artists}\n",
    "original_zorder = {a: a.get_zorder() for a in artists}\n",
    "otu_rabund = [d_rabund.loc[extraction_id][otus].sum() for extraction_id in plotting_order]\n",
    "\n",
    "group_cvrg_included = group_cvrg.loc[group_order.index][group_cvrg.contamination_score < contam_threshold]\n",
    "group_order_included = group_order.loc[group_cvrg_included.index]\n",
    "\n",
    "for inx, d1 in group_order_included.iterrows():\n",
    "    ax.axvline(d1.left - 0.5, color='k', lw=1, linestyle='--')\n",
    "    ax.annotate('({})'.format(inx), xy=(d1.middle, group_cvrg_included.group_max_coverage.max()), ha='center')\n",
    "    ax.annotate('{:0.02}'.format(group_cvrg_included.loc[inx].total_length),\n",
    "                xy=(d1.middle, group_cvrg_included.group_max_coverage.max() * 0.5),\n",
    "                ha='center', rotation=-90)\n",
    "\n",
    "annot = ax.annotate('', xy=(0.02, 0.8), xycoords=\"axes fraction\", rotation=90)\n",
    "ax.set_yscale('symlog', linthreshy=1)\n",
    "fig.tight_layout()\n",
    "\n",
    "def _init():\n",
    "    return artists\n",
    "\n",
    "def _animate(i):\n",
    "    j = i - 1\n",
    "    artists[i].set_linewidth(1)\n",
    "    artists[i].set_alpha(0.9)\n",
    "    artists[i].set_zorder(999)\n",
    "    artists[j].set_linewidth(original_lw[artists[j]])\n",
    "    artists[j].set_alpha(original_alpha[artists[j]])\n",
    "    artists[j].set_zorder(original_zorder[artists[j]])\n",
    "    annot.set_text('{} ({:0.1f}%)'.format(plotting_order[i], otu_rabund[i]*100))\n",
    "    return [artists[i], artists[j], annot]\n",
    "\n",
    "anim = animation.FuncAnimation(fig, _animate, init_func=_init,\n",
    "                               frames=cvrg_norm.shape[0], interval=200, blit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.colors import SymLogNorm\n",
    "\n",
    "colors = [(1, 1, 1), (0, 0.3, 0), (0, 0.7, 0), (0, 1, 0)]  # R -> G -> B\n",
    "n_bins = [3, 6, 10, 100]  # Discretizes the interpolation into bins\n",
    "cmap_name = 'custom1'\n",
    "cm = LinearSegmentedColormap.from_list(cmap_name, colors, N=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_groups = []  #[0, 12, 5]\n",
    "\n",
    "d = (cvrg_norm.groupby(group_assign.group, axis='columns').median()\n",
    "              .loc[:, group_cvrg[lambda x: (x.contamination_score < contam_threshold) &\n",
    "                                           ~x.index.isin(drop_groups)].index])\n",
    "\n",
    "vmin, vmax, cmap, norm = 0, 8, cm, SymLogNorm(linthresh=1, linscale=0.9)\n",
    "\n",
    "sns.clustermap(d[extraction_meta.site == 'UM'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n",
    "sns.clustermap(d[extraction_meta.site == 'UT'], vmin=vmin, vmax=vmax,\n",
    "               col_cluster=False, robust=True, cmap=cmap, norm=norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('res/core.a.mags.d/OTU-1-UM.contigs.list', 'w') as handle:\n",
    "#    for contig_id in group_assign[lambda x: x.group.isin([1, 9, 14, 4, 6, 15])].index:\n",
    "#        print(contig_id, file=handle)\n",
    "        \n",
    "with open('res/core.a.mags.d/OTU-1.contigs.list', 'w') as handle:\n",
    "    for contig_id in group_assign[lambda x: x.group.isin([1, 3, 7, 13, 14, 17, 2, 6, 8, 15, 17, 19])].index:\n",
    "        print(contig_id, file=handle)\n",
    "                \n",
    "#with open('res/core.a.mags.d/OTU-1-UT.contigs.list', 'w') as handle:\n",
    "#    for contig_id in group_assign[lambda x: x.group.isin([1, 9, 14, 3, 15, 16, 19])].index:\n",
    "#        print(contig_id, file=handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "102px",
    "width": "189px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}